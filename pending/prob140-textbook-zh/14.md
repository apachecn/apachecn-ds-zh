# 十四、 中心极限定律

> 原文：[prob140/textbook/notebooks/ch_14](https://nbviewer.jupyter.org/github/prob140/textbook/blob/gh-pages/notebooks/Chapter_14/)
> 
> 译者：[喵十八](https://github.com/Yao544303)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)
> 
> 自豪地采用[谷歌翻译](https://translate.google.cn/)

# 本章依赖的 python

```python
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```


```python
# HIDDEN
def dist_sum(n, probs_0_through_N):
    """Return the distribution of S_n,
    the sum of n i.i.d. copies
    of a random variable with distribution probs_0_through_N
    on the integers 0, 1, 2, ..., N"""
    
    # Find the possible values of S_n
    N = len(probs_0_through_N) - 1   
    values_Sn = np.arange(n*N + 1)
    
    # Find the probailities of those values
    coeffs_X1 = np.flipud(probs_0_through_N)
    pgf_X1 = np.poly1d(coeffs_X1)
    pgf_Sn = pgf_X1**n
    coeffs_Sn = pgf_Sn.c
    probs_Sn = np.flipud(coeffs_Sn)
    
    t = Table().with_columns(
        'Value', values_Sn,
        'Probability', probs_Sn
    )
    return t
```

# 中心极限定理 #

标准差是广为流传的衡量标准之一，此外，还有很多其他的衡量标准。为什么使用标准差？主要的原因是标准差和[正态曲线](https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/7246669?fr=aladdin)之间的联系。为何正态曲线如此重要？本章将回答该问题。

我们将从分析一个独立同分布的概率和开始。并已知其均值和标准差。在本章中，我们将研究分布的形状：当我们可以计算它的精确形状时，我们计算它，当数据量过大时，我们计算一个其近似值。


## 精确分布 ##

我们已经知道如何找到任意两个离散随机变量之和的分布。

$$
P(X+Y = k) = \sum_j P(X=j, Y=k-j)
$$

如果$X$和$Y$是独立的，这简化为成为*离散卷积公式*：

$$
P(X+Y = k) = \sum_j P(X=j)P(Y=k-j)
$$

通过归纳，我们可以将其扩展到任何有限数量的自变量的和。

所以原则上讲，我们知道如何找到$n$个（$n > 1$）独立随机变量的概率分布之和。但是，对于很大的$n$，这种方式很难实现。

在本节中，我们将研究另一种分布求和的方法。正如您将看到的，它更容易实现自动化，但最终也会遇到计算障碍。

### 概率生成函数 ###
设$X$是一个随机变量，对于给定的$N$，其可能取值为$0, 1, 2, \ldots, N$。
为简洁起见，令$p_k = P(X = k)$，其中$k$的取值范围为 0 到$N$。

定义$X$的*概率生成函数*(pgf)为

$$
G_X(s) ~  = ~ \sum_{k=0}^N p_ks^k, ~~~ -\infty < s < \infty
$$

对于具有无限多个非负整数随机变量的扩展，请参阅本节末尾的技术说明。


上面的定义表明对任何$s$，有
$$
G_X(s) ~ = ~ p_0 + p_1s + p_2s^2 + p_3s^3 + \cdots + p_Ns^N
$$

你可以看到$G_X$是一个$N$次多项式，并且$s^k$的系数是$p_k = P(X=k)$。

因此，如果给你一个随机变量的 pgf，你可以通过简单地列出所有的权重和相应的系数来计算出随机变量的分布。

要了解这如何帮助我们找到总和的分布，请观察每一个$s$，$G_X(s)$的期望为

$$
G_X(s) ~ = ~ \sum_{k=0}^N s^kP(X=k) ~ = ~ E(s^X)
$$

因此，如果$X$和$Y$是独立的非负整数随机变量，那么对于每个$s$有

$$
G_{X+Y}(s) ~ = ~ E(s^{X+Y}) ~ = ~ E(s^X s^Y) ~ = ~ E(s^X)E(s^Y)
~ = ~ G_X(s)G_Y(s)
$$

我们已经使用了这样的事实：对于独立的随机变量，其相乘的期望是期望的相乘。

结果表明两个独立随机变量之和的 pgf 是两个 pgf 的乘积。这很容易扩展到两个以上的随机变量，并为独立同分布变量之和的 pgf 产生一个简单的公式。

### 一个独立同分布样本分布之和的 PGF ###
设$X_1, X_2, \ldots, X_n$是分布在$0, 1, 2, \ldots, N$上的独立同分布事件。令$S_n = X_1 + X_2 + \cdots + X_n$，那么$S_n$的 pgf 为：

$$
G_{S_n}(s) ~ = ~ \big{(}G_{X_1}(s)\big{)}^n, ~~~ -\infty < s < \infty
$$

因为$G_{X_1}$是一个$N$次多项式，$G_{S_n}$也是一个$nN$次多项式。与任何 pgf 一样，$s^k$的系数是$k$的概率。也就是说，对于每一个在 0 到$nN$范围内的$k$有

$$
P(S_n = k) = \text{coefficient of } s^k \text{ in } G_{S_n}(s)
$$

我们现在有一个查找$S_n$分布的算法。

- 从$X_1$的 pgf 开始。
- 增加幂至$n$。也就是$S_n$的 pgf。
- 读取$S_n$的 pgf.

精彩！我们完成了！除了实际上这样做涉及将多项式升幂。当数很大时，这是一项艰巨的任务。

幸运的是，正如您将在下一节中看到的那样，`NumPy`使用一组多项式方法来解决问题。

*技术说明。*我们已经为具有有限多个非负整数值的随机变量定义了概率生成函数。该定义可以扩展到具有无限多个非负整数值的随机变量。但在这种情况下，pgf 是一个无限系列，我们必须小心收敛。通常，pdf 是的值域 $|s| \le 1$，这样它就会收敛。 



## NumPy 中的 PGF ##

回忆一下，我们找到$S_n$分布的算法。

- 从$X_1$的 pgf 开始。
- 增加幂至$n$。也就是$S_n$的 pgf。
- 读取$S_n$的 pgf.

在本节中，我们将使用`NumPy`实践此算法。

假设$X_1$的分布为$p_0 = 0.1$，$p_1 = 0.5$，$p_2 = 0.4$。令数组`probs_X1`包含了 0,1,2 的概率。



```python
probs_X1 = make_array(0.1, 0.5, 0.4)
```


```python
dist_X1 = Table().values(np.arange(3)).probability(probs_X1)
Plot(dist_X1)
```


![pngimg/14_02_output_5_0.png)


$X_1$的 pgf 是：
$$
0.1 + 0.5s + 0.4s^2
$$

`NumPy` 以标准的数学方式表示这个多项式，以最高次项开始：

$$
0.4s^2 + 0.5s + 0.1
$$

方法`np.flipud`将概率数组反转为与该系数的顺序一致。`ud`代表“up down”。NumPy 正在考虑将数组作为一个列。`NumPy`考虑将该数组转为一列。


```python
coeffs_X1 = np.flipud(probs_X1)
coeffs_X1
```




    array([ 0.4,  0.5,  0.1])



方法`np.poly1d`以系数数组为参数，构造多项式。方法名中的`1d`代表"一维"。


```python
pgf_X1 = np.poly1d(coeffs_X1)
print(pgf_X1)
```

         2
    0.4 x + 0.5 x + 0.1
    

调用`print`方法，打印出该多项式。在$s$的位置，用$x$代替表示。请记住，最后一项是$x^0$的系数。

现在假设$S_3$是三个$X_1$副本的和。$S_3$的 pgf 是$X_1$pgf 的三次方，并且可以按照您的希望计算。



```python
pgf_S3 = pgf_X1**3
print(pgf_S3)
```

           6        5         4         3         2
    0.064 x + 0.24 x + 0.348 x + 0.245 x + 0.087 x + 0.015 x + 0.001
    

$S_3$中多项式的幂为从 0 到 6，因为$S_3$是三个幂从 0 到 2 的值的副本的和。系数是$S_3$分布的概率。

你可以使用属性`c`输出其“系数”。


```python
coeffs_S3 = pgf_S3.c
coeffs_S3
```




    array([ 0.064,  0.24 ,  0.348,  0.245,  0.087,  0.015,  0.001])



这些是从 6 次到 0 次项的系数。在概率中，更习惯从低到高的顺序来看，所以再使用一次`np.flipud`：



```python
probs_S3 = np.flipud(coeffs_S3)
probs_S3
```




    array([ 0.001,  0.015,  0.087,  0.245,  0.348,  0.24 ,  0.064])



您现在拥有绘制$S_3$的概率直方图所需的输入了


```python
dist_S3 = Table().values(np.arange(7)).probability(probs_S3)
Plot(dist_S3)
```


![pngimg/14_02_output_17_0.png)


### 计算分布$S_n$的函数 ###
我们将结合上面的步骤来创建一个函数`dist_sum`，入参为副本个数$n$和$X_1$的分布，返回值为$n$个$X_1$的副本的和的分布。


```python
def dist_sum(n, probs_0_through_N):
    """Return the distribution of S_n,
    the sum of n i.i.d. copies
    of a random variable with distribution probs_0_through_N
    on the integers 0, 1, 2, ..., N"""
    
    # Find the possible values of S_n
    N = len(probs_0_through_N) - 1   
    values_Sn = np.arange(n*N + 1)
    
    # Find the probailities of those values
    coeffs_X1 = np.flipud(probs_0_through_N)
    pgf_X1 = np.poly1d(coeffs_X1)
    pgf_Sn = pgf_X1**n
    coeffs_Sn = pgf_Sn.c
    probs_Sn = np.flipud(coeffs_Sn)
    
    t = Table().with_columns(
        'Value', values_Sn,
        'Probability', probs_Sn
    )
    return t
```

### $n$次掷骰子游戏的和 ###
在第 3 章中，我们通过列出所有的$6^5$种可能情况，并计算他们从而找到了 5 次掷骰子游戏的和的分布。这种方法难以处理大数据量的情况。让我们看看我们的新方法是否可以找到 10 个骰子总和的分布。

我们必须从单个筛子的分布开始，为此重要的是要记住包含 0 作为 0 个点的概率。否则 pgf 将是错误的，因为`NumPy`不知道它不应该包括 0 次项。



```python
die = np.append(0, (1/6)*np.ones(6))
die
```




    array([ 0.        ,  0.16666667,  0.16666667,  0.16666667,  0.16666667,
            0.16666667,  0.16666667])




```python
Plot(dist_sum(1, die))
```


![pngimg/14_02_output_22_0.png)



```python
Plot(dist_sum(10, die))
```


![pngimg/14_02_output_23_0.png)


### 制作波 ###
10 个筛子和的分布，看上去很符合正态分布。是所有的和都这样么？

要探索这个问题，让$X_1$的分布为$p_1 = p_2 = p_9 = 1/3$


```python
probs_X1 = make_array(0, 1/3, 1/3, 0, 0, 0, 0, 0, 0, 1/3)
```

这是$X_1$的分布。


```python
Plot(dist_sum(1, probs_X1))
```


![pngimg/14_02_output_27_0.png)


$S_{10}$的概率直方图表明“和”并不总是具有平滑的分布。


```python
Plot(dist_sum(10, probs_X1))
```


![pngimg/14_02_output_29_0.png)


$S_{30}$的分布看上去头发乱糟糟的剑龙。


```python
Plot(dist_sum(30, probs_X1))
```


![pngimg/14_02_output_31_0.png)


$S_{100}$的分布是：


```python
Plot(dist_sum(100, probs_X1))
```


![pngimg/14_02_output_33_0.png)


... 非常正态分布了. 

这看起来好像这里有什么定理。在本章的其余部分，我们将研究该定理，该定理是关于大量独立同分布样本之和的近似分布。

请记住，只要`NumPy`能够处理计算,我们的 pgf 方法就能求出有限多个非负整数上的独立同分布样本分布总和的*精确分布*。在上面的例子中， $S_{100}$的 pgf 是一个最高次项为 900 的多项式。`NumPy`处理得很好。


## 中心极限定理 ##

顾名思义，这个定理是概率，统计和数据科学领域的核心。它解释了上一节中出现的正态曲线。

在我们得到定理之前，让我们回顾一下数据 8 和本课程前面的一些事实。

### 标准单位 ###
正如我们之前看到的，随机变量 $X$转换为*标准单位*如下

$$
Z = \frac{X - \mu_X}{\sigma_X}
$$

$Z$以标准差为单位，衡量了$X$离开均值的距离（如，均值 4，标准差 1,5 就离开均值 1 个标准差的距离）。换句话说$Z$表示了$X$高于均值的标准差的数。

按线性函数规则，无论$X$的分布是什么，都有
$$
E(Z) = 0 ~~~ \text{and} ~~~ SD(Z) = 1
$$

### 标准正态曲线 ###
回顾数据 8，标准正常曲线由通常用小写希腊字母 phi，$\phi$表示的函数定义，。

$$
\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}, ~~~ -\infty < z < \infty
$$


```python
# HIDDEN
Plot_norm(x_limits=(-4, 4), mu=0, sigma=1)
plt.xlabel('$z$')
plt.ylabel('$\phi(z)$', rotation=0)
plt.title('Standard Normal Curve');
```


![pngimg/14_03_output_5_0.png)


该曲线关于 0 对称。其拐点在$z=-1$和$z=1$。您在 Data 8 中观察到了这一点并且可以通过微积分证明它。

**术语** 我们将说曲线具有*位置参数* 0 和*比例* 参数 1。我们还将使用术语*平均值*来表示位置和*标准差*来表示比例，类似于标准单位中随机变量的均值和标准差。在本课程的后面，我们将证明这与具有连续值的随机变量的均值和标准差的定义一致。

曲线下的总面积为 1。这需要一些工作来证明。您可能已经在微积分课中看到过它。我们将在课程的后期使用概率方法证明它。


如果是随机变量$X$的分布大致是钟形，那么标准化变量$Z$的分布大致遵循上面的标准正态曲线。

请注意，几乎没有概率落在范围$(-3, 3)之外。回顾一下数据 8 中的一下数据：

- 介于-1 和 1 之间的面积：约 68％
- 介于-2 和 2 之间的面积：约 95％
- 介于-3 和 3 之间的面积：约 99.73％

### 正态曲线 ###
标准正态曲线是这样一*类*正态曲线，由其位置和比例参数参数，及均值和标准差确定。

均值为$\mu$，方差为$\sigma$的正态曲线定义如下：

$$
f(x) ~ = ~ \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}, ~~~ -\infty < x < \infty
$$


```python
# HIDDEN
Plot_norm(x_limits=(-4, 4), mu=0, sigma=1)
plt.xlabel('$x$')
plt.ylabel('$f(x)$', rotation=0)
plt.yticks(np.arange(0, 0.401, 0.05), np.array(7*['']))
plt.xticks(np.arange(-4, 4.1), ['','','$\mu-2\sigma$', '$\mu - \sigma$', '$\mu$', '$\mu+\sigma$','$\mu+2\sigma$',''])
plt.title('Normal Curve, mean $\mu$, SD $\sigma$');
```


![pngimg/14_03_output_8_0.png)


形状看起来与标准正常曲线完全相同。唯一的区别在于轴上的测量尺度。中心现在是$\mu$而不是 0，并且拐点远离中心距离是以$\sigma$为单位而不是 1。

现在给出正态曲线的重要性：

### 中心极限定理  ###
设 $X_1, X_2, \ldots$ 是 i.i.d., 每一个都有均值$\mu$和标准差$\sigma$。令$S_n = X_1 + X_2 + \cdots + X_n$，则有

$$
E(S_n) = n\mu ~~~~~~~~~~ SD(S_n) = \sqrt{n}\sigma
$$

我们还不知道$S_n$的分布的形状。*中心极限定理*（CLT）告诉我们，当$n$很大时，曲线会很平滑。

#### 定理 ####
当$n$很大时，标准分布的和为
$$
\frac{S_n - n\mu}{\sqrt{n}\sigma}
$$

无论$X_i$的分布如何，最终将大致遵循标准正态分布。

换言之，当$n$很大时，无论$X_i$的分布如何，$S_n$的分布与均值$n\mu$和标准差$\sqrt{n}\sigma$有关

中心极限定理是使用标准差对分布进行衡量的主要原因。

究竟当$n$多大时，估计值能够有一个较好的结果？这取决于$X_i$的分布。我们稍后会详细说明。现在，假设我们使用的样本大小足够大，以使正态估计合理。

该定理的证明超出了本课程的范围。但是你已经在数据 8 中进行的模拟以及前一节中计算的总和的精确分布中看到了大量的证据。

#### 示例 ####
假设一个样本为 100 人的重量，是独立同分布的，其均值为 150 磅，标准差为 15 磅。然后所采样的人的总重量的均值为$100 \times 150 = 15,000$，标准差为$\sqrt{100} \times 15 = 150$。

谁在乎一群随机人的总重量？询问那些建造体育馆，电梯和飞机的人。

您可以使用`prob140`方法绘制此分布`Plot_norm`。参数是您希望绘制曲线的间隔，平均值和标准差。


```python
n = 100
mu = 150
sigma = 15

mean = n*mu
sd = (n**0.5)*sigma

plot_interval = make_array(mean-4*sd, mean+4*sd)

Plot_norm(plot_interval, mean, sd)
```


![pngimg/14_03_output_12_0.png)


### 正态曲线下的概率 ###
假设我们想要找到抽样人员的总重量小于 15,100 磅的概率。这大约是下面的黄金区域。这是使用正态曲线的一个估计。

请注意参数`right_end=15100`。这告诉`Plot_norm`其右边界。如果没有指定左端，则最左端视为其左边界。


```python
Plot_norm(plot_interval, mean, sd, right_end=15100)
```


![pngimg/14_03_output_14_0.png)


和以前一样，返回点左边所有概率的函数称为分布的*累积分布函数*（cdf）。`stats.norm.cdf`使用适当的参数，概率不到 75％。


```python
stats.norm.cdf(15100, mean, sd)
```




    0.74750746245307709



为了估计总重量在 14,800 磅到 15,100 磅之间的概率率是多少？现在我们指定两个参数`left_end`和`right_end`：


```python
Plot_norm(plot_interval, mean, sd, left_end=14800, right_end=15100)
```


![pngimg/14_03_output_18_0.png)


阴影面积约为 65.6％.


```python
stats.norm.cdf(15100, mean, sd) - stats.norm.cdf(14800, mean, sd)
```




    0.65629624272720921



### 标准正态 CDF $\Phi$ ###
实际上只有一条正常曲线很重要 - 标准正态曲线。所有其他的都是标准正态曲线通过水平轴的线性变换获得的。因此，通过标准化，可以根据标准正态 cdf 完成上述所有计算，如下所述。

要找到总重量小于 15,100 磅的大概几率，首先将重量标准化，然后使用标准正态 cdf。


```python
z_right = (15100 - mean)/sd

stats.norm.cdf(z_right)  # The standard curve is the default
```




    0.74750746245307709



要找到总重量在 14,800 磅到 15,100 磅之间的概率：


```python
z_left = (14800 - mean)/sd

stats.norm.cdf(z_right) - stats.norm.cdf(z_left)
```




    0.65629624272720921



标准正态 cdf 的常用符号是大写字母$\Phi$，因为它是$\phi$的积分：

$$
\Phi(x) = \int_{-\infty}^x \phi(z)dz, ~~~~ -\infty < x < \infty
$$

这个积分虽然是有限的，但没有封闭形式的公式，可以用算术运算，幂，三角函数，指数和对数函数以及组合来改写。它必须通过数值积分来求近似值。这就是为什么每个统计系统都有内置功能，例如`stats.norm.cdf`提供出色的近似值功能。

标准化标准正态累积分布函数$\Phi$为所有正态曲线下的面积值提供了紧凑的表示法。我们不必对不同的参数使用不同的函数。

在 CLT 的假设下，对于大的值$n$我们有近似值


$$
P(S_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - n\mu}{\sqrt{n}\sigma} \big{)} ~~~ \text{for all } x
$$

正如您在数据 8 中看到的那样，近似值通常在分布的尾部中表现不佳。如果使用 CLT 来逼近尾部区域的概率，请注意近似值可能非常粗糙。

### 二项分布 $(n, p)$ 的估计 ###

一个二项随机分布$(n, p)$ 是$n$个 i.i.d.分布的和。CLT 表明，如果$n$足够大，无论$p$是什么，分布是大致成正态分布的。但我们在第 6 章中说过，如果$n$很大，$p$很小，那么二项分布大致是泊松分布。

那么它到底是正态分布还是泊松分布？这是两个二项式直方图，两者都有大的$n$但有不同的形状。


```python
k1 = np.arange(25, 76)
probs1 = stats.binom.pmf(k1, 100, 0.5)
binom_fair = Table().values(k1).probability(probs1)
Plot(binom_fair)
plt.title('Binomial (100, 0.5)');
```


![pngimg/14_03_output_27_0.png)



```python
k2 = np.arange(0, 11)
probs2 = stats.binom.pmf(k2, 100, 0.01)
binom_biased = Table().values(k2).probability(probs2)
Plot(binom_biased)
plt.title('Binomial (100, 0.1)');
```


![pngimg/14_03_output_28_0.png)


差异是由于分布的扩散。当分布在 0 附近时，泊松近似适用。当扩展较大时，在均值的任一侧存在大量可能值，则可以尝试正态分布。

为了量化这一点，许多文本根据给出了粗略的阈值$n$和$p$，使得，如果$n$大于阈值，那么二项式$(n, p)$大致是正态分布。如果$n$很大，二项分布类似于泊松，意味着$n$没有超过正态分布的阈值。

阈值通常以“标准差$\sqrt{npq}$大于 3” 或“$np$和$nq$都大于 10”来表示，这些不完全一致，但非常相近。

您可以通过比较二项式与相应泊松之间的总变化距离以及二项式与相应法线之间的总变化距离来了解您对这些阈值的看法。然而，在这个过程中，对二项式的法线与泊松近似的选择很少会成为一个问题，因为当$n$和$p$的值都给出时， 如果您对使用哪个有疑问，那么您可以使用确切的二项式概率。

这是二项式（100,0.5）分布和近似正态曲线。曲线的参数是$np = 50$和$\sqrt{npq} = 5$。


```python
Plot(binom_fair)
Plot_norm((25, 75), 50, 5, color='red')
plt.xticks(np.arange(25, 76, 5))
plt.title('Binomial (100, 0.5) and its Normal Approximation');
```


![pngimg/14_03_output_31_0.png)


注意点“$\mbox{mean } \pm \mbox{ SD}$“$= 50 \pm 5$是曲线的拐点。






## 样本均值 ##

中心极限定理的核心是什么？一个答案是它允许我们基于随机样本做出推论，即使我们对总体的分布知之甚少。

在数据 8 中，您看到如果我们想要估计总体的平均值，我们可以基于大随机样本的平均值来构建参数的置信区间。在该过程中，您使用引导程序生成样本均值的经验分布，然后使用经验分布来创建置信区间。你会记得那些经验分布总是钟形的。

在本节中，我们将研究样本均值的概率分布，并表明您可以使用它来构建总体均值的置信区间，而无需进行任何重新采样。

让我们从样本总和开始，我们现在很清楚。回想一下我们的假设和符号：

设$X_1, X_2, \ldots, X_n$ 是一个 i.i.d 采样, 设每一个$X_i$的均值为$\mu$标准差为$\sigma$。设$S_n$是样本总和，即$S_n = \sum_{i=1}^n X_i$。可以得到

$$
E(S_n) = n\mu ~~~~~~~~~~  SD(S_n) = \sqrt{n}\sigma
$$

这些结果意味着随着样本量的增加，样本总和的分布向右移动并变得更加分散。

您可以在下图中看到这一点。该图显示了 5 个筛子的总和和 20 个筛子的总和的分布。分布是精确的，使用本章前面定义的`dist_sum`方法计算。


```python
die = np.append(0, (1/6)*np.ones(6))
dist_sum_5 = dist_sum(5, die)
dist_sum_20 = dist_sum(20, die)
Plots('Sum of 5 dice', dist_sum_5, 'Sum of 20 dice', dist_sum_20)
```


![pngimg/14_04_output_4_0.png)


您可以看到正态分布已经显示为 5 和 20 个样本的总和。

您还可以看到黄金区域不是蓝色区域的四倍，尽管黄金区域中的样本大小是蓝色的四倍。黄金区域高只有蓝色一半，分布是蓝色的两倍。那是因为总和的标准与$\sqrt{n}$成正比。它增长比$n$慢。由于样本量大 4 倍，因此黄金分布的标准差为蓝色的 $\sqrt{4} = 2$倍。

样本的*平均值*表现不同

### IID 样本的均值 ###
设$\bar{X}_n$是样本年均值，即

$$
\bar{X}_n = \frac{S_n}{n}
$$

然后$\bar{X}_n$只是$S_n$的线性变换，所以

$$
E(\bar{X}_n) = \frac{E(S_n)}{n} = \frac{n\mu}{n} = \mu ~~~~ \text{for all }n
$$

样本均值的期望总是总体的均值$\mu$，无论样本大小。因此，无论样本大小如何，样本均值都是总体均值的无偏估计。

样本均值标准差是

$$
SD(\bar{X}_n) = \frac{SD(S_n)}{n} = \frac{\sqrt{n}\sigma}{n} = \frac{\sigma}{\sqrt{n}}
$$

随着样本量的增加，样本均值的变化性降低。因此，随着样本量的增加，样本均值对总体均值的估计更准确。

下图显示了 5 个筛子和 20 个筛子的平均值的分布。两者都以 3.5 为中心，但较大样本的均值分布较窄。您在数据 8 中经常看到这一点：随着样本量的增加，样本均值的分布更集中在总体均值周围。


```python
# HIDDEN
dist_mean_5 = Table().with_columns(
    'Value', dist_sum_5.column(0)/5,
    'Probability', 5*dist_sum_5.column(1)
    )
dist_mean_20 = Table().with_columns(
    'Value', dist_sum_20.column(0)/20,
    'Probability', 20*dist_sum_20.column(1)
    )
Plots('Mean of 5 dice', dist_mean_5, 'Mean of 20 dice', dist_mean_20, width=0.2)
```


![pngimg/14_04_output_7_0.png)


精确是有代价的。样本平均值的标准差随着样本大小的平方根减小。因此，如果要将样本均值的标准差降低 3 倍，则必须将样本量增加$3^2 = 9$倍。

一般结果通常可作为反例进行证明。

#### 平方根法则 ####
如果将样本扩大 n 倍，则样本均值的均值会减少$\sqrt{n}$倍。

### 弱大数定律 ###
样本均值是总体均值的无偏估计，并且当样本较大时，其标准差比较小。因此，大样本的平均值接近总体平均值的概率很高。

这一结论称为*弱大数定律*。

令$X_1, X_2, \ldots, X_n$是 i.i.d.，每一个有均值$\mu$和标准差$\sigma$，令$\bar{X}_n$是样本均值。取一个极小数$\epsilon > 0$，有

$$
P(|\bar{X}_n - \mu| < \epsilon) \to 1 ~~~ \text{as } n \to \infty
$$

也就是说，对于$n$取值很大时，几乎可以确定均值在$\mu \pm \epsilon$的范围内，

为了证明该定律，我命将证明$P(|\bar{X}_n - \mu| \ge \epsilon) \to 0$，这是用切比雪夫不等式很容易求解。

$$
P(|\bar{X}_n - \mu| \ge \epsilon)~ \le ~ \frac{\sigma_{\bar{X}_n}^2}{\epsilon^2} 
~ = ~ \frac{\sigma^2}{n\epsilon^2} ~ \to ~ 0 ~~~ \text{as } n \to \infty
$$

### 相关定律 ###
- **强大数定律。** 这表示在概率为 1 时，样本平均值收敛到极限，并且该极限是常数$\mu$。请参阅[Fields Medalist Terence Tao 撰写的这篇博客文章](https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/)。他陈述了基础标准差可能不存在的情况下的定律情况。请注意，我们的弱大数定律证明方法在这种情况下无效; 结果仍然是正确的，但证据需要更多的探索。
- **小数定律。** 这是[Ladislaus Bortkiewicz](https://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz) (1868-1931)一本书的标题。其中他描述了低概率事件分布的泊松近似。这就是为什么这些注释的第 6.4 节被称为小数定律。
- **平均定律。** 在总体是二元的情况下，这是弱法则的通用名称，样本均值只是样本中成功的比例。在通常的使用中，人们有时会忘记定律是一种限制性陈述。如果你正在抛硬币并连续看到 10 个正面，那么下一个抛正面的机会仍然是 1/2。平均律并没有说你“应该是反面”。它不适用于有限的投掷集。

### 分布的形状 ###
中心极限定理告诉我们，对于大样本，样本均值的分布大致是正态的。样本均值是样本和的线性变换。因此，如果样本总和的分布大致是正态的，则样本均值的分布也大致是正态的，但具有不同的参数。具体来说，对于$n$很大的情况下

$$
P(\bar{X}_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - \mu}{\sigma/\sqrt{n}} \big{)} ~~~~ \text{for all } x
$$




```python
# HIDDEN
Plot_norm(x_limits=(-4, 4), mu=0, sigma=1)
plt.yticks(np.arange(0, 0.401, 0.05), np.array(7*['']))
plt.xticks(np.arange(-4, 4.1),['','','','$\mu - \sigma/\sqrt{n}$', '$\mu$', '$\mu+\sigma/\sqrt{n}$','',''])
plt.title('Approximate Distribution of Sample Mean');
```


![pngimg/14_04_output_12_0.png)



## 置信区间 ##

假设你有一个大的 iid 样本。CLT 意味着有约 95％的概率，样本均值在总体平均值的 2 个标准差距离内。


```python
# HIDDEN
Plot_norm(x_limits=(-4, 4), mu=0, sigma=1, left_end=-2, right_end=2)
plt.yticks(np.arange(0, 0.401, 0.05), np.array(7*['']))
plt.xticks(np.arange(-4, 4.1),['','','$\mu - 2\sigma/\sqrt{n}$', '', '$\mu$', '', '$\mu+2\sigma/\sqrt{n}$',''])
plt.xlabel('Sample Mean')
plt.title('Gold Area: Approximately 95%');
```


![pngimg/14_05_output_3_0.png)


这可以用不同的方式表达：

- 在所有样本集合的约 95％的样本中，样本平均值在*总体平均值$\pm ~ 2 \sigma/\sqrt{n}$*的范围内。

换言之：

- 在所有样本集合的约 95％的样本中， 总体的平均值在*样本均值 $\pm ~ 2 \sigma/\sqrt{n}$*的范围内。

这就是为什么用*样本均值$\pm ~ 2 \sigma/\sqrt{n}$*作为$\mu$的估计间隔。

### $\mu$的置信区间 ###

*样本均值$\pm ~ 2 \sigma/\sqrt{n}$*的区间称之为*参数$\mu$的 95％置信区间*这个区间，拥有一个 95%的*置信水平*。

你可以选择不同的置信水平，比如说 80％。在这个选择下，你的期望区间会更窄。要确切了解中心两侧需要多少标准差的距离，来获得大约 80％的中心区域，您必须找到在标准正态曲线上相应的$z$，如下图所示。


```python
# HIDDEN
Plot_norm(x_limits=(-4, 4), mu=0, sigma=1, left_end=-1.28, right_end=1.28)
plt.yticks(np.arange(0, 0.401, 0.05), np.array(7*['']))
plt.xticks(make_array(-1.28, 0, 1.28),['$-z$', '0', '$z$'])
plt.title('Gold Area: Approximately 80%');
```


![pngimg/14_05_output_6_0.png)


正如您从数据 8 中所知，并且可以在图中看到，间隔从分布的第 10 百分位到第 90 百分位。所以$z$是标准正态曲线的第 90 个百分点，也称为曲线的“90％点”。`scipy`方法会调用`ppf`并将 f 分位数的十进制值作为其参数。


```python
stats.norm.ppf(.9)
```




    1.2815515655446004



因此，总体的大约 80％置信区间意味着总体均值$\mu$在"样本均值 $\pm ~ 1.28\sigma/\sqrt{n}$"范围内。


让我们仔细校验，2 是$z$的一个很好的取值，这意味着 95％的置信区间。该$z$我们需要的是 97.5％的点数：


```python
stats.norm.ppf(.975)
```




    1.959963984540054



那是$z = 1.96$，这就是我们一直使用的 2。这个值已经足够了，但是$z = 1.96$也常用于构建 95％置信区间。

### 一般定义 ###
设$\lambda$是一个置信水平，令$z_\lambda$代表了这样一个分位数，使得正态曲线中，$(-z_\lambda, ~ z_\lambda)$包含了$\lambda$%区域。在上面的例子中，$\lambda$的值是 80， $z_\lambda$的值是 1.28。

当$n$足够大时，有

$$
\frac{\lambda}{100} ~ \approx ~ 
P(\bar{X}_n \in \mu ~ \pm ~ z_\lambda \sigma/\sqrt{n}) ~ = ~
P(\mu \in \bar{X}_n ~ \pm ~ z_\lambda \sigma/\sqrt{n})
$$

随机区间$\bar{X}_n ~ \pm ~ z_\lambda \sigma/\sqrt{n}$被称为*总体均值$\mu$的$\lambda$%置信区间*。这意味着，大约有$\lambda$%的概率，该随机区间包含$\mu$。

不同级别的置信区间之间的唯一区别是$z$的选择，这取决于置信水平。另外两个组成是样本均值和标准差。

### 复习数据 8 中的示例 ###
让我们回到数据 8 中非常熟悉的一个例子：1,174 对母亲及其新生儿的随机样本。


```python
baby = Table.read_table('baby.csv')
```


```python
baby
```




<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Birth Weight</th> <th>Gestational Days</th> <th>Maternal Age</th> <th>Maternal Height</th> <th>Maternal Pregnancy Weight</th> <th>Maternal Smoker</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>120         </td> <td>284             </td> <td>27          </td> <td>62             </td> <td>100                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>113         </td> <td>282             </td> <td>33          </td> <td>64             </td> <td>135                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>128         </td> <td>279             </td> <td>28          </td> <td>64             </td> <td>115                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>108         </td> <td>282             </td> <td>23          </td> <td>67             </td> <td>125                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>136         </td> <td>286             </td> <td>25          </td> <td>62             </td> <td>93                       </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>138         </td> <td>244             </td> <td>33          </td> <td>62             </td> <td>178                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>132         </td> <td>245             </td> <td>23          </td> <td>65             </td> <td>140                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>120         </td> <td>289             </td> <td>25          </td> <td>62             </td> <td>125                      </td> <td>False          </td>
        </tr>
    </tbody>
        <tr>
            <td>143         </td> <td>299             </td> <td>30          </td> <td>66             </td> <td>136                      </td> <td>True           </td>
        </tr>
    </tbody>
        <tr>
            <td>140         </td> <td>351             </td> <td>27          </td> <td>68             </td> <td>120                      </td> <td>False          </td>
        </tr>
    </tbody>
</table>
<p>... (1164 rows omitted)</p>



第三栏包括母亲的年龄。让我们为总体中母亲的平均年龄构建大约 95％的置信区间。我们在 Data 8 中使用 bootstrap 完成了这项工作，因此我们有了能够进行比较的结果。

因为我们的数据来自大型随机样本，我们可以应用本节的方法。


```python
n = 1174
ages = baby.column('Maternal Age')

samp_mean = np.mean(ages)
samp_mean
```




    27.228279386712096



可以发现样本的$\bar{X}_n$值是 27.23。我们知道$n = 1174$，所以，我们需要总体的标准差$\sigma$然后就可以完成我们的计算。

但是，我们当然不知道总体的标准差$\sigma$。

所以，我们使用数据来估计$\sigma$，当然，这个估计存在一些误差，但它除以 $\sqrt{n}$后，误差会被缩小。请记住，我们的方法依赖于 CLT，仅在$n$很大时有效。

$\sigma$的估计大约是 5。82 年。


```python
sigma_estimate = np.std(ages)
sigma_estimate
```




    5.8153604041908968



一个总体的 95%置信区间是$(26.89, 27.57)$。


```python
samp_mean - 2*sigma_estimate/(n**0.5), samp_mean + 2*sigma_estimate/(n**0.5)
```




    (26.888831911866099, 27.567726861558093)



不需要 bootstrapping 了! 

现在让我们比较两种方法的结果。调用 Data 8 的`bootstrap_mean`方法。


```python
def bootstrap_mean(original_sample, label, replications):
    
    """Displays approximate 95% confidence interval for population mean.
    original_sample: table containing the original sample
    label: label of column containing the variable
    replications: number of bootstrap samples
    """
    just_one_column = original_sample.column(label)
    n = len(just_one_column)
    means = make_array()
    for i in np.arange(replications):
        bootstrap_sample = np.random.choice(just_one_column, size=n)
        resampled_mean = np.mean(bootstrap_sample)
        means = np.append(means, resampled_mean)
        
    left = percentile(2.5, means)
    right = percentile(97.5, means)
    
    resampled_means = Table().with_column(
    'Bootstrap Sample Mean', means
    )
    resampled_means.hist(bins=15)
    print('Approximate 95% confidence interval for population mean:')
    print(np.round(left, 2), 'to', np.round(right, 2))
    plt.plot(make_array(left, right), make_array(0, 0), color='yellow', lw=8);
```

让我们为总体平均值构建 95％置信区间的 bootstrap。我们将使用 5000 引导样本，就像我们在 Data 8 中所做的那样。


```python
bootstrap_mean(baby, 'Maternal Age', 5000)
```

    Approximate 95% confidence interval for population mean:
    26.9 to 27.57
    


![pngimg/14_05_output_25_1.png)


bootstrap 置信区间与我们使用正态近似得到的区间（26.89,27.57）基本相同。

正如我们在数据 8 中所做的那样，我们观察到样本中母亲年龄的分布远非正态分布：


```python
baby.select('Maternal Age').hist()
```


![pngimg/14_05_output_28_0.png)


但是，样本均值的经验分布，显示为前一个单元格的输出，大致为钟形。这是因为由中心极限定理可得，大样本的平均值的概率分布是近似正态的。
