# 二十一、Beta 和二项

> 原文：[prob140/textbook/notebooks/ch21](https://nbviewer.jupyter.org/github/prob140/textbook/tree/gh-pages/notebooks/Chapter_21/)
> 
> 译者：[吕哲](https://github.com/lvzhetx)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)
> 
> 自豪地采用[谷歌翻译](https://translate.google.cn/)


```python
# HIDDEN
from datascience import *
from prob140 import *
%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import numpy as np
```


## Beta 分布和二项 ##

这一章中的关于 Beta 和二项的联系的相关知识，在机器学习领域有着重要意义。在前面的章节中，你已经初探了一些他们之间的联系。在本章中，我们会推广这些结果。

我们将要研究的实验有两个部分。

 - 通过 beta 分布选取 $p$ 的值
 - 掷枚硬币来确定被我们选取的概率 $p$

我们将会看到先验信息和数据是如何与后来的硬币正面向上的分布相联系。在观察 n 次掷硬币的结果以后，我们将会预下一次掷硬币的结果。我们将会在 n 次随机硬币实验中观测得到硬币正反面的分布情况，并且与以后的硬币投掷结果比例进行比对。

在实验室中，当预先不知道可能性的大小的时候，你就可以把这些理论运用到聚类模型中去。
```python
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```

```python
# HIDDEN
def plot_prior_and_posterior(r, s, n, k):
    p = np.arange(0, 1, 0.01)
    prior = stats.beta.pdf(p, r, s)
    posterior = stats.beta.pdf(p, r+k, s+n-k)
    plt.plot(p, prior, lw=2, color='gold', label = 'Prior: beta (r, s)')
    plt.plot(p, posterior, lw=2, color='darkblue', label = 'Posterior: beta(r+k, s+n-k)')
    plt.legend(bbox_to_anchor=(1.6, 1.02))
    ymax = max(max(prior), max(posterior))
    plt.ylim(-0.3, ymax+0.1)
    plt.xlim(0, 1)
    plt.scatter(r/(r+s), -0.1, marker='^', s=40, color='gold')
    plt.scatter((r+k)/(r+s+n), -0.1, marker='^', s=40, color='darkblue')
    plt.scatter(k/n, -0.02, s=30, color='red')
    plt.xlabel('$p$')
    plt.title('Prior, and Posterior Given $S_n = k$ (red dot at $k/n$)');
```
## 预测和更新 ##

设 $X$ 是一个满足 Beta 概率密度的随机变量。给定 $X=p$ ，抛掷正面概率为$p$的硬币 $n$ 次，并观察结果。基于正面向上的次数的结果，我们将

* 确定 $X$ 的后验分布
* 预测 $(n+1)$ 次正面向上的概率

### Beta 先验 ###

对于正整数 $r$ 和 $s$ ，通过研究均匀随机变量 $(0, 1)$ 的独立同分布次序统计量，我们可以得到 beta$(r, s)$ 的概率密度函数。

$$
f(x) = \frac{(r+s-1)!}{(r-1)!(s-1)!} x^{r-1}(1-x)^{s-1}, ~~~ 0 < x < 1
$$

Beta 族可以扩展到正数但不一定是整数的参数 $r$ 和 $s$ 。这种情况只是可能，因为你在实验中还观察到了以下两个事实：

* Gamma 函数是阶乘函数的连续扩展。
* 如果 $r$ 是一个正整数，那么 $\Gamma(r) = (r-1)!$ 。

所以，为了满足 $r$ 和 $s$ 只需要是正数而不一定是整数，我们重新定义 beta $(r, s)$ 概率密度函数：

$$
f(x) = \begin{cases} \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)} x^{r-1}(1-x)^{s-1}, ~~~ 0 < x < 1 \\ 0 ~~~~~~~~~~~ \text{otherwise} \end{cases}
$$

我们不会证明这个函数的定义域已经包括了 1 ，但这一结论确实是真实可信的，因为我们已经观察到了这一结果对于整数参数值是有效的。
为了简化表示，我们将 β$(r，s)$ 概率密度函数中的常数表示为 $C(r，s)$ 。

$$ 
C(r, s) ~ = ~ \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)} $$

那么 β$(r，s)$ 的概率密度函数，就可以简化表示为:

$C(r, s)x^{r-1}(1-x)^{s-1}$ for $x \in (0, 1)$ 。

Beta 分布通常用于随机比例数据的建模。在上一个章节中，你已经看到了 beta$(1, 1)$ 的分布情况(或称为均匀分布），这种分布已经被用来给随机投掷硬币建模。

如你所见，如果我们知道我们抛出的硬币正面的概率 $p$ ，那么他就是随机独立试验，但是如果我们不知道 $p$ 的值是多少的时候，那投掷的情况就不再独立了。因为比如说，第一次投掷硬币的结果会告诉我们一些关于 $p$ 值的信息，这个信息又会反馈出一些第二次投掷的可能结果的概率。

接下来我们会通过一个硬币向上概率的 beta $(r, s)$ 先验信息推广我们刚才的那些结果。

### 实验 ###

我们令 $X$ 具有 beta $(r, s)$ 分布，这就是 $X$ 的先验信息，$f_X$ 为先验信息的概率函数。那么可以得到
$$f_X(p) ~ = ~ C(r, s)p^{r-1}(1-p)^{s-1}, ~~~~ 0 < p < 1$$
令 $X = p$ ，设 $I_1, I_2, \ldots$ 服从以 $(p)$ 为参数的独立同伯努利分布（译注：同时满足独立同分布以及伯努利分布）。也就是说，给定概率 $X = p$，重复抛一个正面概率为$p$的硬币，然后记录结果为$I_1, I_2, \ldots$
再设 $S_n = I_1 + I_2 + \cdots + I_n$ ，其中 $S_n$ 为 n 次独立重复投硬币中，正面向上的次数。所以，当 $S_n$ 中的 $X$ 满足 $X = p$ 时，$S_n$ 的条件分布就是以 $(n, p)$ 为参数的二项分布

### 更新：给定 $S_n$ 的后验分布 ###

在开始实验之前，回顾一下，我们之前已经提到，$X$ 具有满足以 $(r, s)$ 为参数的 beta 先验。接下来我们需要找到在给定条件 $S_n = k$ 时的 $X$ 的后验信息来更新掷了$n$次以后的先验信息。
我们在前面已经看到，后验分布的概率密度与之前投掷正面的次数成正比，对于 $0 < p < 1$ ,

$$
f_{X|S_n=k}(p)\propto{C(r, s) p^{r-1}(1-p)^{s-1} \cdot \binom{n}{k} p^k (1-p)^{n-k}}
$$

$$
\propto ~ p^{r+k-1}(1-p)^{s + (n-k) - 1} 
$$

这之中，因为 $C(r, s)$ 和 $\binom{n}{k}$ 不包括 $p$ ，所以你有时候可以看到 beta 密度函数是 $f_{X \mid S_n = k} (p) ~ = ~ C(r+k, s+n-k) p^{r+k-1}(1-p)^{s + n - k - 1}, ~~~ 0 < p < 1$ 的形式。
当然，这个 beta 后验密度函数就很好去记忆了。我们先验信息开始；接着用观察到的正面向上的次数更新第一个参数；我们用观察到的反面向上的次数去更新第二个参数。

#### 共轭先验 ####

硬币正面向上概率的先验分布是来自 beta 族。正面向上概率的后验分布中，已经给定了正面向上的次数，是另一种 beta 密度函数。beta 先验分布和似然函数相乘（还有一个系数）就可以得到后验分布。因此，beta 族被称为是对于二项分布的共轭先验。在这个概念中，后验分布在这里是与先验分布同一个族的（与先验分布拥有相同的函数形式)。

### 最大后验概率估计：后验众数 ###

最大后验估计正面概率实际上就是用的后验分布函数来进行的。如果 $r$ 和 $s$ 都比 1 大，那么 $X$ 的后验分布的众数则是：

$$
\frac{r+k-1}{r+s+n-2}
$$

### 后验均值 ###

当给定 $S_n = k$ 时，$X$ 的后验均值就是 beta 后验的期望，在 $n$ 比较大的情况下，和后验众数差不多：

$$
E(X \mid S_n = k) ~ = ~ \frac{r+k}{r+s+n}
$$

我们可以在一个例子中验证和这个结果，假设 $X$ 的先验分布是 beta $(5, 3)$，因此计算出先验均值就是 $E(X) = 5/8 = 0.625$ 。现在，假设我们给定 $S_{100} = 70$，这样一来，$X$的后验分布就是 beta $(75, 33)$，后验均值就是 $75/108 = 0.694$ 。

下面的图展示了两种有对应均值的概率密度函数。红点是正面向上的实际观测比例。

再进行相同的操作，保持 $r = 5$ 以及 $s = 3$ 但是把 $n$ 改成 10、 $k$ 改成 7。然后把 $n$ 改成 1000、$k$ 改成 $700$，再试一次。接着观察图中概率为 0.7 的位置。注意观察样本大小是如何在 0.7 周围汇集的，我们稍后将介绍汇集的原因。

当然，也可以试试其他 $n$ 和 $k$ 值，比如观察到的值和先验均值相差很大的情况。

```python
# Prior: beta (r, s)
# Given: S_n = k

# Change the values
r = 5
s = 3
n = 100
k = 70

# Leave this line alone
plot_prior_and_posterior(r, s, n, k)
```
![Data Influences Ipriorimg/21-1-1.png)

你可以在图中看到，数据是如何影响先验分布的。先验分布一般会聚集在先验均值附近。上图中均值是 0.625 ，但当给定 100 次投掷中有 70 次正面向上的时候，后验均值就是 0.694，这就和我们的直接观察结果 0.7 很相近了。

先验均值的公式显示了，当 $n$ 很大的时候，先验均值就会很接近直接观察结果中正面向上的概率。当给定 $S_n = k$ 的时候，后验均值的公式就是：

$$
E(X \mid S_n = k) ~ = ~ \frac{r + k}{r + s + n}
$$

因此，当 $S_n$ 作为一个随机变量的时候，后验均值就是：

$$
E(X \mid S_n) ~ = ~ \frac{r + S_n}{r + s + n}
$$

随着投掷次数 $n$ 增大，正面向上的次数也会随着变多。所以分子就会主要受到 $S_n$ 的影响，而分母主要受到 $n$ 的影响。因为 $r$ 和 $s$ 是常数，所以当 $n$ 很大的时候，后验均值就会接近于 $S_n/n$.

### 预测：给定 $S_n$ 预测 $S_{n+1}$ 的分布 ###

如你在之前的章节中所见，掷一枚硬币正面向上的可能性是它随机概率的估计值，现在把这个放到我们当前的背景中来思考：

$$
P(S_1 = 1) ~ = ~ P(\text{第一次投掷正面向上}) ~ = ~ E(X) ~ = ~ \frac{r}{r+s}
$$

假设我们有前 $n$ 次投掷的结果，这其中 $k$ 次是正面向上。给定 $S_n = k$ ，那么 $S_{n+1}$ 的可能值就是 $k$ 或者 $k+1$ 。我们现在就能够使用更新后的 $X$ 的分布，与上面同理:

$$
P(S_{n+1} = k+1 \mid S_n = k) ~ = ~ P(\text{投掷硬币第 } n+1 \text{ 次是正面向上 } \mid S_n = k)~ = ~ E(X \mid S_n = k) ~ = ~ \frac{r+k}{r + s + n}
$$

我们可以使用完整的方程来描述 $P(S_{n+1} = k \mid S_n = k)$ ，当 $S_n = k$ 给定的时候，$S_{n+1}$ 的条件分布函数为：

$$
S_{n+1} =
\begin{cases} 
k ~~~~~~~~ \text{ with probability } (s + n - k)/(r + s + n) \\
k+1 ~~ \text{ with probability } (r+k)/(r + s + n)
\end{cases}
$$

换句话说，当知晓前 $n$ 次的投掷结果的时候，第 $n+1$ 次是尾部的概率与 $s$ 乘反面次数之积成正比，第 $n+1$ 次是头部的概率与 $r$ 乘正面次数之积成正比。
你可以把 $\{ S_n: n \ge 1 \}$ 看做一个马尔科夫链（译注: 描述了一种状态序列，其每个状态值取决于前面有限个状态）,但是值得注意的是，过渡概率不是以时间为次序的，因为其中还包含了参数 $n$。


```python
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```

## $β$-二项分布 ##

和上节一样， $X$ 具有 beta $(r, s)$ 先验，给定 $X = p$，并且让 $S_n$ 作为前 $n$ 次投掷一枚正面概率为 $p$ 的硬币正面向上的次数。
我们在上一节中进行的所有计算都是在 $S_n = k$ 的条件下进行的，但是在实际运用中我们其实对这种概率求解的情况并不常见。但它是 $X$ 的后验概率密度函数的常数项之一，使得这个后验概率密度函数的积分为 1。
我们现在可以用两种形式来表达后验概率密度函数，从而得到 $P(S_n = k)$ 
通过回忆之前的内容，我们可以得到 beta $(r+k, s+n-k)$ 概率密度函数：

$$
f_{X \vert S_n=k} (p) ~ = ~ C(r+k, s+n-k)p^{r+k-1}(1-p)^{s+n-k-1},0<p<1
$$

再使用贝叶斯规则：

$$
f_{X \vert S_n=k} (p) ~ = ~ \frac{C(r, s) p^{r-1}(1-p)^{s-1} \cdot \binom{n}{k} p^k (1-p)^{n-k}}{P(S_n = k)},0<p<1
$$

两个方程联立得：

$$ 
\frac{C(r, s) \binom{n}{k}}{P(S_n = k)} ~ = ~ C(r+k, s+n-k) 
$$

### $β$-二项概率 ###

对于 $k$ 在 $0$ 到 $n$ 的范围中时:

$$
P(S_n = k) ~ = ~  \binom{n}{k} \frac{C(r, s)}{C(r+k, s+n-k)}
$$

此处, $C(r,s)$ 是 beta $(r, s)$ 概率密度函数中的常量，并且等于：

$$
C(r, s) ~ = ~ \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}
$$

这种离散分布就被称为以 $r,s,n$ 为参数的$β$-二项分布。它可以表示一个硬币在 $n$ 次投掷中正面向上的次数分布，其中，这个硬币正面向上的概率来自 beta $(r,s)$ 分布。

对于 $(r, s)$ 值等于 $(1, 1)$ 时会比较特殊，这种情况下 $X$ 的先验信息是均匀先验，此时 $S_n$ 可以化简为：

$$
P(S_n = k ) ~ = ~ \frac{n!}{k!(n-k)!} \cdot \frac{1!}{0!0!} \cdot \frac{k!(n-k)!}{(n+1)!} ~ = ~ \frac{1}{n+1}
$$

可以观察到，$ P(S_n = k ) $ 的表达式中没有 $k$。所以我们可以得到一个结论，如果 $p$ 是在 0 到 1 之间均匀地选取，掷一个正面概率为 $p$ 的硬币 $n$ 次，那么他的正面向上次数的概率就是 $\{ 0, 1, 2, \ldots, n\}$ 上的均匀分布。

如果 $p$ 是在 0 到 1 之间非均匀地选取，那么对于条件分布 $S_n$ ， 给定 $p$， 它的值就是以 $(n, p)$ 为参数的二项分布。但 $S_n$ 如果是无条件分布，那么它的分布就是均匀的。

### 通过积分来检验 ###

如果你愿意，你可以通过调整 $X$ 的值来直接获得 $S_n$ 的值。
$$
P(S_n = k) ~= \int_0^1 P(S_n = k \mid X = p)f_X(p)dp 
$$
$$
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ = ~ \int_0^1 \binom{n}{k} p^k(1-p)^{n-k}C(r, s)p^{r-1}(1-p)^{s-1}dp 
$$
$$
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ = ~ \binom{n}{k} C(r, s) \int_0^1 p^{r+k-1}(1-p)^{s+n-k-1} dp
$$
$$
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ = ~ \binom{n}{k} C(r, s) \frac{1}{C(r+k, s+n-k)}
$$


### 期望 ###

给定 $X = p$ ，$S_n$ 的条件分布就是以 $(n, p)$ 为参数的二项分布，因此，

$$
E(S_n \mid X = p) ~ = ~ np
$$

或者，等价的，

$$
E(S_n \mid X) ~ = ~ nX
$$

再通过迭代可以得到，

$$
E(S_n) ~ = ~ E(nX) ~ = ~ nE(X) ~ = ~ n\frac{r}{r+s}
$$

因此，再 $n$ 次投掷中期望比例就是，

$$
E\big{(} \frac{S_n}{n} \big{)} ~ = ~ \frac{r}{r+s}
$$

这就是 $X$ 先验分布的期望。

在下一节中我们会研究随机比例更长远的情况。

### 尾注 ###

可以从前面看到，无条件概率 $P(S_n = k)$ 出现在给定 $S_n$ 的 $X$ 后验密度函数的分母中。由于使用共轭先验带来的简化，我们可以用几种不同的方法计算分母。但是计算过程往往很复杂，特别是在高维情况下。在一些更加高级的其他课程中会讲述解决这种问题的方法。

```python
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```

## 长期比例 ##

暂时先固定 $p$ ，考虑一系列符合独立同伯努利分布的投掷 $I_1, I_2, \ldots$。通过弱大数定律可得，当尝试的次数逐渐增大的时候，成功次数的比例会接近于 $p$ 。但是强大数定律会更加正确和准确：随着次数增加，样本成功的比例收拢于 $p$ 概率为 1（译注：意为在样本数很大的时候，成功次数的比例无限趋近于 $p$ ，概率为 1 意思是说利用极限的定义，对任何 $ε$ ，$|p-μ|＜ε$ 恒成立）。

强大数定律的推导已经超出了这门课程的范围，但是我们依然可以通过模拟来观察大量投掷的结果。

我们会建立一个名为  `binomial_proportion` 的函数，它输入的参数为 $n$ 和 $p$ ，最后返回的结果为一个由序列 $\{ \frac{S_k}{k}: k = 1, 2, \ldots, n\}$ 组成的数组，其中 $S_k = I_1 + I_2 + \cdots + I_k$ 。这个序列就是在 $n$ 次尝试中硬币正面向上的比例。

```python
def binomial_proportions(p, n):
    bernoulli = stats.binom.rvs(1, p, size=n)
    return np.cumsum(bernoulli)/np.arange(1, n+1)
```
```python
binomial_proportions(0.5, 4)
```

结果：array([ 0.        ,  0.        ,  0.33333333,  0.5       ])

上面的数组就是投掷一个正反面概率相同硬币四次的比例序列。请自己尝试，看看你能否计算出相应正反面对应的序列。

函数 `plot_binomial_proportions` 是一个绘制函数，可以显示出比例。它的第一个参数是包含所有 $p$ 的数组，第二个参数尝试的次数。对于数组中每一个 $p$ ，函数 `binomial_proportion` 都会计算 $n$ 次独立同伯努利分布中成功次数的比例，并且在 $k = 1, 2, \ldots , n$ 点对应的位置画出来。

我们把每一次绘制都叫做一条观测比例路径。

```python
def plot_binomial_proportions(p_array, n):
    for p in p_array:
        plt.plot(np.arange(1, n+1), binomial_proportions(p, n), lw=2)
    plt.ylim(-0.05, 1.05)
    plt.xlabel('$k$')
    plt.title('Proportion of Successes in $k$ Trials');
```

我们现在可以使用`plot_binomial_proportions` 来模拟 1000 个正反面比例相同的硬币抛掷，并且记录下正面向上比例的曲线。

```python
two_fair_coins = make_array(0.5, 0.5)
plot_binomial_proportions(two_fair_coins, 1000)
```
![plot_binomial_proportionsimg/21-3-1.png)

当实验的次数很少的时候，曲线拨动略大，但是当次数逐渐增大，它渐渐收敛在 0.5 左右。

那为什么 $p$ 需要被组成一个数组输入而不是一个数字？请耐心继续往下看。通过创建一个具有 10 个数，每个数都是 1/6 的数组来生成 10 条投掷一个骰子 1000 次，其中有 6 次正面向上的曲线。

```python
ten_dice = (1/6)*np.ones(10)
plot_binomial_proportions(ten_dice, 1000)
```
![leveloffimg/21-3-2.png)

这些曲线最终在 1/6 左右变得平稳。

通常来说，正面向上的比例会收敛于固定值 $p$

### 随机化 $p$ ###

现在，让我们用随机数来初始化 $p$ 的值。我们先从一个简单的先验开始，让 $p$ 取值为等可能性的 0.2 或者 0.7 中任意一个。
那么这个过程就变成了：

 - 等可能性地从 0.2 和 0.7 中选择一个 $p$ 的值
 - 抛以这个选定 $p$ 为正面向上概率的硬币，并且记录下比例曲线

为了能够生成这个过程的曲线，我们需要从随机选取 $p$ 的值开始。这就是为什么我们必须指定 $p$ 的值为一个数组，而不是一个数字。这个数组中的元素个数也就是我们需要绘制曲线的数量。

下面有 10 条曲线，请注意此处我们用的 `np.random.choice` 来为 $p$ 随机取值。

```python
random_p = np.random.choice([0.2, 0.7], size = 10, p = [0.5, 0.5])
plot_binomial_proportions(random_p, 1000)
```

![leveloffimg/21-3-3.png)

现在你可以看到大约一半的曲线平稳收敛于 0.2，还有一半平稳收敛于 0.7。这是因为，在值得选取中 0.7 和 0.2 的选择是等可能性的。其中选择 0.7 的时候，曲线会收敛于 0.7，选择 0.2 的时候，曲线会收敛于 0.2 ，这和我们之前的描述向对应。

你观察到的结果就是，如果 $n$ 很大，观测比例在 $n$ 时候的分布会趋向于 $p$ 的先验分布。

### β先验 ###

我们先从均值先验说起。在前面的章节中，我们让 $X$ 在 (0,1) 上均匀分布，给定 $X=p$ 进行独立同伯努利实验。下面就是这个过程的 10 条曲线，每条曲线由 1000 次投掷尝试组成。记住，在 (0,1) 上的均值分布和在 (1,1) 上的 $β$ 分布是一样的。

```python
p_array = stats.beta.rvs(size = 10, a = 1, b = 1)
plot_binomial_proportions(p_array, 1000)
```
![uniformimg/21-3-4.png)

在上图的右边，是 1000 次实验中正面向上次数比例分布的表示点。如果进行更多次，你会发现这个分布是大致一样的。

事实上，从最下面的曲线到最上面的曲线，你已经看到了满足 (0,1) 上的独立同均匀分布的随机变量的顺序统计量。

下面是 15 条根据 $β(2,8)$ 分布来选取的概率 $p$ 值的实验正面向上次数的比例分布的曲线。

```python
p_array = stats.beta.rvs(size = 15, a = 2, b = 8)
plot_binomial_proportions(p_array, 1000)
```

![15pathimg/21-3-5.png)

下面的图绘制了 $β(2,8)$ 的概率密度函数。其中取值的概率主要分布在 0 到 0.6 之间。这个结果与上图里面收敛的数值相对应。

![leveloffimg/21-3-6.png)

### Beta 先验，Beta 极限 ###

这一部分图表明，在很大试验次数下的正面次数比例分布和正面次数的先验分布是一样的。
如果先验分布是 beta$(r, s)$ ,那么正面次数的极限分布也会是 beta $(r, s)$。
下面的模拟证实了这一个结论。它由 10000 次下面的重复步骤组成：

 - 从 beta $(2, 8)$ 分布中选取 $p$ 的值
 - 掷选定概率为 $p$ 的硬币 1000 次，并且记录下正面向上的比例

下面的直方图显示了 10000 次模拟的比例分布，红色的曲线是 $(2, 8)$ 概率密度曲线

```python
proportions = make_array()
for i in range(10000):
    chosen_p = stats.beta.rvs(size = 1, a = 2, b = 8)
    obs_prop = stats.binom.rvs(size = 1, n = 1000, p = chosen_p)/1000
    proportions = np.append(proportions, obs_prop)

Table().with_column('Proportion of Successes', proportions).hist(bins=20)
x = np.arange(0, 1.01, 0.01)
y = stats.beta.pdf(x, a = 2, b = 8)
plt.plot(x, y, color='red', lw=2)
plt.title('Beta $(2, 8)$ Prior, and Proportion of Successes at $n = 1000$');
```
![endimg/21-3-7.png)
