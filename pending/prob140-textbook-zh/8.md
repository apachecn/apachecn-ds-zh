# 8.期望  

> 译者：[PEGASUS1993](https://github.com/PEGASUS1993)  [GG-yuki](https://github.com/GG-yuki)

## 8.0 期望概述
关于概率是如何分布在变量所有可能值的，随机变量的分布给我们提供了详细信息。但是通常我们只是想大致了解它们分布在数字线的位置。换句话来说，我们只是想知道分布中心的位置。  
经历过多次“期中”考试的任何学生都知道，“中间”和“中心”这样的词语没有独特的含义。本章是关于随机变量的一种特殊“中心”。  
### 示例  
```
from datascience import *  
from prob140 import *  
import numpy as np  
import matplotlib.pyplot as plt  
plt.style.use('fivethirtyeight')  
%matplotlib inline
```
## 8.1 定义  

随机变量 X 的期望，可以表示为 E(X)，是由 X 经过概率加权所有可能值的平均值。这可以用两种等效的方式来计算。  
在 X 的域上：
  
　　　　　　　　　　　　E(X) = \sum_{\text{all }x} xP(X=x) 

在 X 的范围内：　　

　　　　　　　　　　　　E(X)=∑(all x)xP(X=x)

 
解释说明：  
如果 X 具有有限多个可能的值，则上式的总和总是很好定义并且是有限的。如果 X 是有相当多的取值，比如由 1,2,3...索引的值，那么我们需要更加小心以确保公式可以得到被明确定义的数值。我们后续将很快涉及到这个问题。现在，我们先假设总和已经被明确定义。  
假设总和定义明确，可以直接表明这两个公式会给出相同的答案。 显示它的一种方法是在所有不同的结果ω中，通过 X（ω）的不同值对第一个求和公式中的项进行分组。  
第二个公式通常作为期望的“定义”给出，但第一个公式可以有助于理解期望的特性。 特别是，它表明如果两个随机变量具有相同的分布，那么它们也应该具有相同的期望。  
### 重心
假设 X 具有以下给出的分布：  
示例：  
```
x = np.arange(1, 6)  #创建 array([1, 2, 3, 4, 5])  
probs = make_array(0.15, 0.25, 0.3, 0.2, 0.1)  
example_dist = Table().values(x).probability(probs)  
Plot(example_dist)
```


   
![](https://i.imgur.com/0L64UxZ.png)  
然后通过 X 的范围上的公式，我们得到 E（X）= 2.85。  
```
ev_X = sum(x*probs)  
ev_X
```  
2.8499999999999996  
你也可以调用 prob14 中的 ev 函数来计算期望 E(X):  
```
example_dist.ev()
```  
2.8500000000000005  
期望我们通常也成为期望值，因此不论是该函数的名称还是我们的名称 ev_V，都是可以这么称呼的。但是请注意期望值不一定是随机变量的可能值。例如该随机变量的可能值就不是 2.85.  
但那么期望值代表着什么呢？要想看到这一点，我们要使用 Plot 中的 show_ev = True 的参数，来可视化期望 E(X).  
```Plot(example_dist, show_ev=True) ``` 
![](https://i.imgur.com/nFNFMj9.png)


##  
如果你已经研究了一些物理学，你会发现我们用于期望的公式与系统的重心公式是相同的，其中权重等于从 1,2,3,4,5 这些可能值相应的概率。  
因此，假设直方图是由纸板或一些刚性材料制成，并想象试图在水平轴上某处固定的铅笔尖上找到平衡点。 你需要将铅笔保持在 2.85 才能达到平衡。  
期望可以看作是物理意义上的分布中心：它是分布的重心或质心。
### 长期平均成本曲线  
当您在相同条件下一次又一次地生成变量时，您还可以将期望视为随机变量的长期平均值。适用于 prob140 分配对象的 sample_from_dist 方法允许您这样做。它随机抽样，从分布中替换，并返回一组采样值。 参数是样本大小。  
您可以使用 emp_dist 函数将模拟值数组转换为分布对象，可以将其与 Plot 和其他 prob140 函数一起使用。 Plot 的 show_ave = True 参数可以显示模拟值的平均值。  
### 示例  
``` 
simulated_X = example_dist.sample_from_dist(10000)  
emp_dist_X = emp_dist(simulated_X)  
Plot(emp_dist_X, show_ave=True)  
plt.title('Empirical Distribution');  
```
![](https://i.imgur.com/nxIBBxW.png)  

X 的 10000 个模拟值的平均值非常接近 E(X)，但不完全相等。  


`np.mean(simulated_X) `

2.8502000000000001  

这是由于您可以在经验直方图中看到：它看起来非常像 X 的概率直方图。大约 15％的模拟值是 1，大约 20％是 2，依此类推，所以平均值非常接近 2.85。   
两个直方图的相似性是因为您在 Data8 中看到的平均定律，我们将在此课程中正式建立 。  
现在我们有几种考虑期望的方法，让我们看看为什么它具有如此重要的意义。 我们将从直接使用定义开始计算一些期望。在后续章节中，我们将开发更强大的方法来计算和使用期望。  
### 唯一性  
这个小例子值得写出来因为它一直被使用。假设随机变量 X 实际上是一个常数 c，即假设 P（X = c）= 1。 然后 X 的分布将其所有质量放在单个值 c 上，并且 E（X）=c⋅1= c。 我们只写 E（c）= c。  
###伯努利和指标  
如果 X 服从伯努利（p）分布，那么 P（X = 1）=p 和 P（X = 0）= 1-p。那么  

　　　　　　　　　　　　　　　E(X)=0⋅(1−p) + 1⋅p = p  　　
如上所述，零/一值随机变量是其他变量的构建块，它们被称为指标。  
假设 A 可以是任何事件，然后 A 的指标是随机变量 I<sub>A</sub>，如果 A 发生则为 1，如果 A 不发生则为 0。这样 I<sub>A</sub>就服从伯努利(P(A)) 分布，可以通过 E(I<sub>A</sub>)=P(A)该式计算。因此，每个概率都是一种期望。我们将在后面的部分中大量使用它。  
```
X = [0, 1]    
qp = [0.75, 0.25]    
bern_1_3 = Table().values(x).probability(qp)    
Plot(bern_1_3, show_ev=True)    
plt.title('Bernoulli (0.25)')   
```  
![](https://i.imgur.com/8JE9Jq8.png)

### 整数区间的一致性  
设 a 和 b 为两个整数，使 a <b。如果 X 在整数 a，a + 1，a + 2，...，b 上具有均匀分布，那么根据对称性，E(X)应该是在 a 和 b 中间位置的。这就是可以将概率直方图平衡的地方。所以有
　　　　　　　　　　　　　　$$ E(X) = \frac{a+b}{2} $$
例如，如果 X 在 1,2，...，n 上具有均匀分布，那么  
　　　　　　　　　　　　　　　　　　　　E(X)=(n+1)/2　　
这种情况的一个例子是，如果 X 是骰子的一个的斑点数，那么 E(X)=3.5。  

如果 X 在 0,1,2，...，n 上是均匀的，那么 E(X)=n/2。  
```
x = np.arange(10)
probs = 0.1*np.ones(10)
unif_10 = Table().values(x).probability(probs)
Plot(unif_10, show_ev=True)
plt.title('Uniform on Integers 0, 1, 2, ..., 9')
```
![](https://i.imgur.com/w0J2Qpb.png)

### 泊松分布

设 X 具有泊松分布(μ)，则  

　　　　　　　![](https://i.imgur.com/6vOMG96.png)

现在我们要对泊松分布的参数进行一个重要的新解释。我们之前看到它的接近形式，现在我们知道这也是分布的平衡点或期望，我们可以使用符号μ代表“均值”。  
```
k = np.arange(15)
poi_2_probs = stats.poisson.pmf(k, 2)
dist_poi_2 = Table().values(k).probability(poi_2_probs)
Plot(dist_poi_2, show_ev=True)
plt.title('Poisson (2)')
```
![](https://i.imgur.com/1Cn6hm9.png)
### 存在性
如果 X 有相当多的值，那么用于定义期望的总和是无限的，因此被限制为求部分和。但并非所有部分和的序列都是有限的，因此并非所有随机变量都有期望。实际上，当和是绝对收敛时，E（X）才被很好地定义：  
$$ E(X) = \sum_{\text{all }x} xP(X=x) ~~~~ \text{provided } \sum_{\text{all }x} |x|P(X=x) < \infty $$    
对于这个水平的课程来说，这有点技术性，在 Pro140 中，你几乎永远不必处理不存在的期望。请记住，期望并不总是有限的，甚至不是很明确。
这里有一个例子，你可以看到期望不能是有限的。 首先注意序列 1 / 2n，n = 1,2,3，...是概率分布，即通过对几何系列求和得到 1。  
　　　　　　![](https://i.imgur.com/8cXoicc.png)　　
现在假设随机变量Ｘ具有 2,4,8,16 ...这些值，因此对于 n=1,2,3，...，P（Ｘ=2n）= 1/ 2n。然后对于每个可能的值Ｘ，乘积 xＰ（Ｘ= x）= 1。如果你试图添加无限多的 1，唯一合理的答案是无限。　　
当分布具有“质量漂移至无穷大”的速率时，就会发生这种期望的问题，这时的速率在概率直方图水平轴方向的任何位置上无法达到平衡。  
## 8.2 可加性  
```
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
import math
from scipy import stats
from scipy import misc
```
在简单的情况下，通过插入定义来计算期望可以起作用，但通常它可能很麻烦或缺乏洞察力。 可以证实，计算期望的最有力结果不是定义。它看起来相当 innocuous。  
### 可加性 
设 X 和 Y 是在相同概率空间上定义的两个随机变量，那么  
　　　　　　　　Ｅ(X+Y) = E(X) + E(Y)  
在我们更仔细地研究这个结果之前，请注意我们事先假设所有期望是存在的; 我们将在本课程中始终贯彻这个假设。  
现在注意到没有关于 X 和 Y 之间关系的假设。它们可以是相关或独立的。无论如何，总和的期望是期望的总和。 这使得结果强有力。  
从 X + Y 的定义和域空间的期望定义可以很容易地看出可加性。首先注意随机变量 X + Y 是由(X+Y)(ω)=X(ω)+Y(ω) for all ω∈Ω定义的函数。  
因此，“由概率加权的 X + Y 的值”可写为(X+Y)(ω)⋅P(ω)=X(ω)P(ω)+Y(ω)P(ω)。  
从两方面对所有ω∈Ω求和，以证明期望的可加性。  
通过归纳，可加性扩展到任何有限数量的随机变量。如果 X1，X2，...，Xn 是在相同概率空间上定义的随机变量，那么 E(X1+X2+⋯+Xn)=E(X1)+E(X2)+⋯+E(Xn)。  
如果你试图找到一个期望，那么使用可加性的方法是将随机变量写成一个简单变量的总和，这些变量的期望是你知道的或者可以很容易地计算出来的。 本节的其余部分包含此技术的示例。  
### 样本总和
设 X1，X2，...，Xn 是服从均匀分布为μ的数字总体中随机抽取的样本，并且使样本总和为 Sn=X1+X2+⋯+Xn。 然后无论是否抽取样品，每个 Xi 都具有与种群相同的分布。如果样品被抽取的话，这显然是正确的，正如我们在前面的章节中看到的那样，如果样品没有被抽取，那么就是对称的。  
因此和是否抽取样本是无关的，（待校对） E(Sn)=E(X1)+E(X2)+⋯+E(Xn)=nμ  
### 线性函数规则
设 X 为随机变量，期望为 E（X），并且对于某一常数 a，设 Y = aX。 例如，当您将随机长度的单位从英寸更改为厘米时，则 a = 2.54(注：1 英寸(in)=2.54 厘米(cm))。  

## 8.3 函数期望   
一旦我们开始使用随机变量作为估计量，我们就会想知道估计值离期望值的差值有多大。例如，我们可能想知道一个随机变量$X$离数字10有多大差值。我们可以定义一个关于$X$的函数$Y$那么
$$Y=| X-10 |$$
它并不是一个线性函数。要找到$E（Y）$，我们需要更多的方法。自始至终，我们将假定我们讨论的所有期望值都是明确的。
这一节是关于如何寻找一个在分布确定情况下，求取随机变量函数的期望值。
在下面的内容中，我们设$X$为一个随机变量，其分布（包括它的期望值）是已知的。

### 线性函数规则

设常量$a$和$b$，对于线性函数$Y=aX+b$而言。在前面的部分中，我们展示了
$$E（Y）=aE（X）+b$$
这包括$a=0$的情况，这种情况下$Y$恒定为$b$，因此期望值为$b$。

### 非线性函数规则

现在让$Y=g（X）$，其中$g$代表任意函数。记住$X$是$\Omega$上的函数。所以定义随机变量$Y$的函数是一个组合：
$$Y（\omega）=（g\circ X）（\omega）~~~~~~~~~~~~~\text{for}\omega\in\omega$$

这允许我们以三种等效方式书写$E（Y）$：
### 在$Y$的范围内	
$$ E(Y) = \sum_{\text{all }y} yP(Y=y) $$
### 在域$\Omega$的范围内
$$ E(Y) = E(g(X)) = \sum_{\omega \in \Omega} (g \circ X) (\omega) P(\omega) $$
### 在$X$的范围内
$$ E(Y) = E(g(X)) = \sum_{\text{all }x} g(x)P(X=x) $$
如上所示，分组简单明了，所有的形式都是等价的。
第一个形式看起来最简单，但有一个问题：首先需要找到$P（Y=Y）$。
第二种形式涉及了很多不必要的细节。
第三种形式是常用的形式。它使用已知的$X$分布。要找到$E（Y）$，、需要$Y=g（X）$中的函数$g$：
—————取$x$的一般值$x$。
—————应用$g$到$x$；则此$g（x）$是$Y$的一般值。
—————将$g（x）$乘以$P（X=x）$，这是已知的。
—————为所有$x$执行此操作并相加，总和记为$E（Y）$。
这个方法最重要的一点是，我们不必先找到$Y$的分布，这节省了我们很多的工作。我们来看看在一些例子中，我们的方法是如何工作的。

### Example 1: $Y = |X-3|$

设$X$服从之前我们用过的分布。

```
x = np.arange(1, 6)
probs = make_array(0.15, 0.25, 0.3, 0.2, 0.1)
dist = Table().values(x).probability(probs)
dist = dist.relabel('Value', 'x').relabel('Probability', 'P(X=x)')
dist
```

x|P(X=x)
1|0.15
2|0.25
3|0.3
4|0.2
5|0.1

设$g$函数为$g（x）=| x-3 |$，设$Y=g（x）$。换句话说，$Y=| X-3 |$。
要计算$E（Y）$，首先必须得到一列数值，即将$X$映射到$Y$后的值：

```
dist_with_Y = dist.with_column('g(x)',
np.abs(dist.column('x')-3)).move_to_end('P(X=x)')

dist_with_Y
```

x|g(x)|P(X=x)
1|2|0.15
2|1|0.25
3|0|0.3
4|1|0.2
5|2|0.1

要获得$E（Y）$，找到适当的加权平均值：将g（x）和P（X=x）相乘，然后将计算所得的值相加得到总和。计算表明，$E（Y）=0.95$。

```
ev_Y = sum(dist_with_Y.column('g(x)') * dist_with_Y.column('P(X=x)'))
ev_Y
```

```
0.94999999999999996
```

### Example 2: $Y = \min(X, 3)$
设$X$与之前的例子中取值相同，但现在设$Y=\min（X，3）$。我们要求出E（Y）。我们知道X$的分配情况：

```
dist
```

x|P(X=x)
1|0.15
2|0.25
3|0.3
4|0.2
5|0.1

要求出$E（Y）$，我们可以逐行计算，并通过$x$的值计算出$\min（x，3）$的值，然后加权平均：

```
ev_Y = 1*0.15 + 2*0.25 + 3*0.3 + 3*0.2 + 3*0.1
ev_Y
```

```
2.45
```

### Example 3: 泊松变量$X$的$E(X^2)$
设$X$具有Poisson$（\mu）$分布。你将在下一个部分中看到，$E（X^2）$的值是非常有用的。根据我们的非线性函数法则，
$$ E(X^2) = \sum_{k=0}^\infty k^2 e^{-\mu} \frac{\mu^k}{k!} $$
这个式子很难简化。$k=0$的条件是0。在$k\ge 1$的每个条件中，分子中的一个$k$和分母中的一个$k$化解，分子中的另一个$k$仍然存在。如果这个因子$k$是$-1$，那么它就可以化解分母中的$k-1$。
这将会推导出以下计算。无论$X$是什么，如果我们知道$E（X）$的值并且可以找到$E（X（X-1））$的值，那么我们可以使用加法来找到$E（X^2）$，如下所示：
$$ E(X(X-1)) = E(X^2 - X) = E(X^2) - E(X) $$
so $$ E(X^2) = E(X(X-1)) + E(X) $$
让我们看看是否可以通过应用非线性函数规则求解$E（X（X-1））$。
\begin{align*} E(X(X-1)) &= \sum_{k=0}^\infty k(k-1) e^{-\mu} \frac{\mu^k}{k!} \\ \\ &= e^{-\mu} \mu^2 \sum_{k=2}^\infty \frac{\mu^{k-2}}{(k-2)!} \\ \\ &= e^{-\mu}\mu^2 e^\mu \\ \\ &= \mu^2 \end{align*}
我们知道$E(X) = \mu$, 所以
$$ E(X^2) = \mu^2 + \mu $$
注意$E（X^2）>（E（X））^2$。这是一个普遍的真实例子。在后面，我们将看到它为什么重要。
现在，作为练习，看看你是否可以推导出$E（X（X-1）（X-2））$，以及$E（X^3）$。
